{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuser/Desktop/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-03 17:41:07.978106: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-03 17:41:08.201891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-03 17:41:08.898243: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, Audio\n",
    "from transformers import AutoProcessor, WhisperModel, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Audio Data\n",
    "list_of_audio_files = [\"data/sub/De95Osq7p1c_trimmed_segment_1.wav\", \"data/sub/De95Osq7p1c_trimmed_segment_2.wav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained('whisper-medium', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperModel.from_pretrained('whisper-medium', local_files_only=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting Adaptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio\n",
    "from transformers import AutoProcessor, WhisperModel, AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class TranslateModel(pl.LightningModule):\n",
    "    def __init__(self, audio_encoder=\"./whisper-medium\", llm=\"./sea-lion-7b-instruct\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Device\n",
    "        self.device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load the Whisper model and processor\n",
    "        print(\"Loading Audio Encoder\")\n",
    "        self.audio_processor = AutoProcessor.from_pretrained(audio_encoder, local_files_only=True)\n",
    "        # self.audio_encoder = WhisperModel.from_pretrained(audio_encoder, local_files_only=True).to(self.device_type)\n",
    "\n",
    "        # Define the Adaptor\n",
    "        self.adaptor = torch.nn.Linear(1024, 4096)  # Do we need bias?\n",
    "\n",
    "        # Load the LLM and its tokenizer\n",
    "        print(\"Loading LLM\")\n",
    "\n",
    "        self.generation_kwargs = {\n",
    "            \"do_sample\": False,  # set to true if temperature is not 0\n",
    "            \"temperature\": None,\n",
    "            \"max_new_tokens\": 256,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.7,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "        }\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            llm, \n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "\n",
    "        # self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "        #     llm,\n",
    "        #     trust_remote_code=True,\n",
    "        #     device_map=\"auto\",\n",
    "        #     local_files_only=True\n",
    "        # )\n",
    "\n",
    "        #self.prefix_embeddings = self.embed_prompt_tokens(\"### USER:\\nTranslate the following to English. \")\n",
    "        #self.suffix_embeddings = self.embed_prompt_tokens(\" \\n\\n### RESPONSE:\\n\")\n",
    "\n",
    "\n",
    "    def forward(self, list_audio_filepaths):\n",
    "        # Encode Audio\n",
    "        # (batch_size, 1500, 1024)\n",
    "        audio_embeddings = self.process_and_encode_audio(list_audio_filepaths)\n",
    "\n",
    "        # Adapt audio embeddings\n",
    "        adapted_audio_embeddings = self.adaptor(audio_embeddings).to()\n",
    "\n",
    "        # Concat audio embeddings with prompt\n",
    "        input_embeddings = torch.cat([self.prefix_embeddings.unsqueeze(0), adapted_audio_embeddings, self.suffix_embeddings.unsqueeze(0)], dim=1)\n",
    "\n",
    "        # Feed into LLM\n",
    "        tokenised_output = self.llm.generate(\n",
    "            input_embeds = input_embeddings,\n",
    "            **self.generation_kwargs\n",
    "        )\n",
    "\n",
    "        # Get translated output\n",
    "        translated_output = self.tokenizer.decode(\n",
    "            tokenised_output[0], \n",
    "            skip_special_tokens= True\n",
    "        )\n",
    "\n",
    "        return translated_output\n",
    "\n",
    "    def process_and_encode_audio(self, list_audio_filepaths):\n",
    "        print(\"Loading Dataset\")\n",
    "\n",
    "        def prepare_dataset(batch):\n",
    "            audio = batch[\"audio\"]\n",
    "            batch[\"input_features\"] = self.audio_processor.feature_extractor(\n",
    "                audio[\"array\"],\n",
    "                sampling_rate=audio[\"sampling_rate\"],\n",
    "                return_tensors='pt'\n",
    "            )['input_features'][0]\n",
    "            return batch\n",
    "\n",
    "        audio_dataset = Dataset.from_dict({\n",
    "            \"audio\": list_audio_filepaths\n",
    "        })\n",
    "\n",
    "        \n",
    "        audio_dataset = audio_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "        print(audio_dataset[0])\n",
    "\n",
    "        # Maps the audio files into Huggingface Dataset Format\n",
    "        print(\"Mapping\")\n",
    "        audio_dataset = audio_dataset.map(prepare_dataset)\n",
    "\n",
    "        \n",
    "        print(\"Mapped\")\n",
    "        inputs = torch.tensor(audio_dataset['input_features']).to(self.device_type)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "        # # Ensuring No Gradient Updates during Encoding\n",
    "        # with torch.no_grad():\n",
    "        #     encoder_outputs = self.audio_encoder.encoder(inputs, output_hidden_states=True)\n",
    "\n",
    "        # return encoder_outputs.last_hidden_state\n",
    "    \n",
    "    # def embed_prompt_tokens(self, string):\n",
    "    #     tokens = self.tokenizer(string, return_tensors=\"pt\")\n",
    "    #     token_embeddings = self.llm.transformer.wte(tokens['input_ids'])\n",
    "    #     return token_embeddings\n",
    "    \n",
    "\n",
    "    # def training_step(self, batch, batch_idx):\n",
    "    #     # Define the training step logic\n",
    "    #     inputs, target = batch\n",
    "    #     output = self(inputs, target)\n",
    "\n",
    "    #     # Return loss\n",
    "    #     pass\n",
    "\n",
    "    # def configure_optimizers(self):\n",
    "    #     # Define the optimizers and schedulers\n",
    "    #     pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Audio Encoder\n",
      "Loading LLM\n"
     ]
    }
   ],
   "source": [
    "translate = TranslateModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/sub/De95Osq7p1c_trimmed_segment_1.wav',\n",
       " 'data/sub/De95Osq7p1c_trimmed_segment_2.wav']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset\n",
      "{'audio': {'path': 'data/sub/De95Osq7p1c_trimmed_segment_1.wav', 'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00323486,\n",
      "        0.00119019, -0.00064087]), 'sampling_rate': 16000}}\n",
      "Mapping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 18.58 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8276, -0.8276, -0.8276,  ..., -0.8276, -0.8276, -0.8276],\n",
       "         [-0.8276, -0.8276, -0.8276,  ..., -0.8276, -0.8276, -0.8276],\n",
       "         [-0.8276, -0.8276, -0.8276,  ..., -0.8276, -0.8276, -0.8276],\n",
       "         ...,\n",
       "         [-0.8276, -0.8276, -0.8276,  ..., -0.8276, -0.8276, -0.8276],\n",
       "         [-0.8276, -0.8276, -0.8276,  ..., -0.8276, -0.8276, -0.8276],\n",
       "         [-0.8276, -0.8276, -0.8276,  ..., -0.8276, -0.8276, -0.8276]],\n",
       "\n",
       "        [[ 0.2992,  0.0724,  0.1169,  ..., -0.8499, -0.8499, -0.8499],\n",
       "         [ 0.2086,  0.0056,  0.0539,  ..., -0.8499, -0.8499, -0.8499],\n",
       "         [ 0.1671,  0.0290,  0.0586,  ..., -0.8499, -0.8499, -0.8499],\n",
       "         ...,\n",
       "         [-0.0828,  0.0063, -0.0152,  ..., -0.8499, -0.8499, -0.8499],\n",
       "         [-0.1165, -0.1588, -0.1714,  ..., -0.8499, -0.8499, -0.8499],\n",
       "         [-0.1584, -0.2315, -0.2529,  ..., -0.8499, -0.8499, -0.8499]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "translate.process_and_encode_audio(list_of_audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset = Dataset.from_dict({\n",
    "            \"audio\": list_of_audio_files\n",
    "        })\n",
    "\n",
    "audio_dataset = audio_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': tensor([[[ 0.2992,  0.0724,  0.1169,  ..., -0.8499, -0.8499, -0.8499],\n",
       "         [ 0.2086,  0.0056,  0.0539,  ..., -0.8499, -0.8499, -0.8499],\n",
       "         [ 0.1671,  0.0290,  0.0586,  ..., -0.8499, -0.8499, -0.8499],\n",
       "         ...,\n",
       "         [-0.0828,  0.0063, -0.0152,  ..., -0.8499, -0.8499, -0.8499],\n",
       "         [-0.1165, -0.1588, -0.1714,  ..., -0.8499, -0.8499, -0.8499],\n",
       "         [-0.1584, -0.2315, -0.2529,  ..., -0.8499, -0.8499, -0.8499]]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for row in audio_dataset:\n",
    "    input = processor(\n",
    "        row[\"audio\"][\"array\"],\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "        sampling_rate=16000\n",
    "    )\n",
    "\n",
    "input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = audio_dataset[0]\n",
    "\n",
    "inputs = processor(\n",
    "    row[\"audio\"][\"array\"],\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    "    sampling_rate=16000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': tensor([[[-0.8276, -0.8276, -0.8276,  ..., -0.8276, -0.8276, -0.8276],\n",
       "         [-0.8276, -0.8276, -0.8276,  ..., -0.8276, -0.8276, -0.8276],\n",
       "         [-0.8276, -0.8276, -0.8276,  ..., -0.8276, -0.8276, -0.8276],\n",
       "         ...,\n",
       "         [-0.8276, -0.8276, -0.8276,  ..., -0.8276, -0.8276, -0.8276],\n",
       "         [-0.8276, -0.8276, -0.8276,  ..., -0.8276, -0.8276, -0.8276],\n",
       "         [-0.8276, -0.8276, -0.8276,  ..., -0.8276, -0.8276, -0.8276]]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
