{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuser/Desktop/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-05 09:52:15.727095: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-05 09:52:15.759316: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-05 09:52:16.315395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import librosa\n",
    "from datasets import Dataset, Audio\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Audio and Transcript Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>reference</th>\n",
       "      <th>trimmed_segment_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2OF_N9xQOAc</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.118</td>\n",
       "      <td>Right now I m in Singapore</td>\n",
       "      <td>data/sub/2OF_N9xQOAc_trimmed_segment_1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2OF_N9xQOAc</td>\n",
       "      <td>3.118</td>\n",
       "      <td>4.909</td>\n",
       "      <td>Most people when they come to Singapore</td>\n",
       "      <td>data/sub/2OF_N9xQOAc_trimmed_segment_2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2OF_N9xQOAc</td>\n",
       "      <td>4.909</td>\n",
       "      <td>6.129</td>\n",
       "      <td>They come to eat dim sum</td>\n",
       "      <td>data/sub/2OF_N9xQOAc_trimmed_segment_3.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2OF_N9xQOAc</td>\n",
       "      <td>6.129</td>\n",
       "      <td>7.498</td>\n",
       "      <td>Visit Merlion</td>\n",
       "      <td>data/sub/2OF_N9xQOAc_trimmed_segment_4.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2OF_N9xQOAc</td>\n",
       "      <td>7.498</td>\n",
       "      <td>8.827</td>\n",
       "      <td>Visit Universal</td>\n",
       "      <td>data/sub/2OF_N9xQOAc_trimmed_segment_5.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>b4o5YC_wMXM</td>\n",
       "      <td>1019.264</td>\n",
       "      <td>1022.082</td>\n",
       "      <td>Please press like share and subscribe</td>\n",
       "      <td>data/sub/b4o5YC_wMXM_trimmed_segment_348.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>b4o5YC_wMXM</td>\n",
       "      <td>1022.107</td>\n",
       "      <td>1025.358</td>\n",
       "      <td>If you want me to take you somewhere</td>\n",
       "      <td>data/sub/b4o5YC_wMXM_trimmed_segment_349.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>b4o5YC_wMXM</td>\n",
       "      <td>1025.383</td>\n",
       "      <td>1027.521</td>\n",
       "      <td>to eat or do any activities</td>\n",
       "      <td>data/sub/b4o5YC_wMXM_trimmed_segment_350.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>b4o5YC_wMXM</td>\n",
       "      <td>1027.546</td>\n",
       "      <td>1029.461</td>\n",
       "      <td>you can leave me a comment</td>\n",
       "      <td>data/sub/b4o5YC_wMXM_trimmed_segment_351.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>b4o5YC_wMXM</td>\n",
       "      <td>1029.587</td>\n",
       "      <td>1031.468</td>\n",
       "      <td>Until next time Bye bye</td>\n",
       "      <td>data/sub/b4o5YC_wMXM_trimmed_segment_352.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>597 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_name  start_time  end_time  \\\n",
       "0    2OF_N9xQOAc       0.000     3.118   \n",
       "1    2OF_N9xQOAc       3.118     4.909   \n",
       "2    2OF_N9xQOAc       4.909     6.129   \n",
       "3    2OF_N9xQOAc       6.129     7.498   \n",
       "4    2OF_N9xQOAc       7.498     8.827   \n",
       "..           ...         ...       ...   \n",
       "592  b4o5YC_wMXM    1019.264  1022.082   \n",
       "593  b4o5YC_wMXM    1022.107  1025.358   \n",
       "594  b4o5YC_wMXM    1025.383  1027.521   \n",
       "595  b4o5YC_wMXM    1027.546  1029.461   \n",
       "596  b4o5YC_wMXM    1029.587  1031.468   \n",
       "\n",
       "                                   reference  \\\n",
       "0                 Right now I m in Singapore   \n",
       "1    Most people when they come to Singapore   \n",
       "2                   They come to eat dim sum   \n",
       "3                              Visit Merlion   \n",
       "4                            Visit Universal   \n",
       "..                                       ...   \n",
       "592    Please press like share and subscribe   \n",
       "593     If you want me to take you somewhere   \n",
       "594              to eat or do any activities   \n",
       "595               you can leave me a comment   \n",
       "596                  Until next time Bye bye   \n",
       "\n",
       "                             trimmed_segment_path  \n",
       "0      data/sub/2OF_N9xQOAc_trimmed_segment_1.wav  \n",
       "1      data/sub/2OF_N9xQOAc_trimmed_segment_2.wav  \n",
       "2      data/sub/2OF_N9xQOAc_trimmed_segment_3.wav  \n",
       "3      data/sub/2OF_N9xQOAc_trimmed_segment_4.wav  \n",
       "4      data/sub/2OF_N9xQOAc_trimmed_segment_5.wav  \n",
       "..                                            ...  \n",
       "592  data/sub/b4o5YC_wMXM_trimmed_segment_348.wav  \n",
       "593  data/sub/b4o5YC_wMXM_trimmed_segment_349.wav  \n",
       "594  data/sub/b4o5YC_wMXM_trimmed_segment_350.wav  \n",
       "595  data/sub/b4o5YC_wMXM_trimmed_segment_351.wav  \n",
       "596  data/sub/b4o5YC_wMXM_trimmed_segment_352.wav  \n",
       "\n",
       "[597 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_files_in_directory(directory):\n",
    "    file_list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        # Only pick up files with .txt extensions (transcript)\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_list.append(filename.replace(\".txt\", \"\"))\n",
    "    return file_list\n",
    "\n",
    "def get_reference_df(directory, audio_txt_file):\n",
    "    txt_file_path = os.path.join(directory, audio_txt_file + \".txt\")\n",
    "    columns = [\"start_time\", \"end_time\", \"reference\"]\n",
    "    # Read the text file into a DataFrame\n",
    "    df = pd.read_csv(txt_file_path, sep=\"\\t\", header=None, names=columns, quoting=3)\n",
    "\n",
    "    # Add file name\n",
    "    df.insert(0, 'file_name', pd.Series([audio_txt_file] * len(df)))\n",
    "\n",
    "    # Remove quotation marks\n",
    "    df['reference'] = df['reference'].apply(lambda x : x.replace('\"',\"\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def trim_wav_by_timestamps(directory, wav_file_name, reference_df):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    output_dir = \"data/sub/\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    wav_file = os.path.join(directory, wav_file_name + \".wav\") # get into data file\n",
    "    \n",
    "    # Load the WAV file\n",
    "    audio = AudioSegment.from_wav(wav_file)\n",
    "    \n",
    "    def trim_segments(row):\n",
    "        start_ms = float(row['start_time']) * 1000  # Convert start time to milliseconds\n",
    "        end_ms = float(row['end_time']) * 1000      # Convert end time to milliseconds\n",
    "        trimmed_segment = audio[start_ms:end_ms]\n",
    "    \n",
    "        return trimmed_segment\n",
    "    \n",
    "    # Iterate over timestamps and trim the audio\n",
    "    for i, row in reference_df.iterrows():\n",
    "        trimmed_segment = trim_segments(row)\n",
    "        output_file = os.path.join(output_dir, wav_file_name + \"_\" f\"trimmed_segment_{i+1}.wav\")\n",
    "        trimmed_segment.export(output_file, format=\"wav\")\n",
    "        reference_df.at[i, 'trimmed_segment_path'] = output_file\n",
    "    \n",
    "    return reference_df\n",
    "\n",
    "def filter_english_subs(reference_df):\n",
    "    # Helper function that is applied across the rows to filter english text only\n",
    "    \n",
    "    def filter_english_only(text):\n",
    "        # Define a regex pattern to match English words\n",
    "        english_pattern = re.compile(r'\\b[A-Za-z]+\\b')\n",
    "        # Find all English words in the text\n",
    "        english_words = english_pattern.findall(text)\n",
    "        # Join the English words into a single string\n",
    "        english_text = ' '.join(english_words)\n",
    "        return english_text\n",
    "\n",
    "    reference_df['reference'] = reference_df['reference'].apply(filter_english_only)\n",
    "\n",
    "    return reference_df\n",
    "\n",
    "def get_combined_audio_table(directory, file_names):\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file_name in file_names:\n",
    "        # Reads the transcript dataframe which has the start_time, end_time of each transcript\n",
    "        reference_df = get_reference_df(directory, file_name)\n",
    "\n",
    "        # Retain only English translations in the transcript (reference) column\n",
    "        reference_df = filter_english_subs(reference_df)\n",
    "\n",
    "        # Trims all the .wav file according to the subtitles start_time and end_time\n",
    "        reference_df = trim_wav_by_timestamps(directory, file_name, reference_df)\n",
    "        \n",
    "        # Append the processed DataFrame to the combined DataFrame\n",
    "        combined_df = pd.concat([combined_df, reference_df], ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "directory = os.path.join(os.getcwd(), \"data/test/\")\n",
    "file_names = list_files_in_directory(directory)\n",
    "df_test = get_combined_audio_table(directory, file_names)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_append_csvs(directory):\n",
    "    # Get list of all CSV files in the directory\n",
    "    all_files = glob.glob(os.path.join(directory, \"*.csv\"))\n",
    "    \n",
    "    # List to hold all DataFrames\n",
    "    df_list = []\n",
    "    \n",
    "    # Read each CSV file and append to the list\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file, index_col= 0)\n",
    "        df_list.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "df_train = read_and_append_csvs('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[~df_train['eng_reference'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[~df_train['thai_reference'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop('meteor_scores', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and Trimming to 28 seconds of Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of audio-transcription pairs: 12090.383999999995\n"
     ]
    }
   ],
   "source": [
    "segment_duration = df_train.apply(lambda x : x['end_time'] - x['start_time'], axis = 1)\n",
    "\n",
    "print(\"Total length of audio-transcription pairs:\", segment_duration.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "# Function to combine WAV files with padding and split into multiple files if necessary\n",
    "def combine_wav_files_with_split(padding_duration_ms, max_duration_seconds, csv_df, output_dir):\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    combined = AudioSegment.silent(duration=0)  # Start with an empty audio segment\n",
    "    padding = AudioSegment.silent(duration=padding_duration_ms)  # Create padding segment\n",
    "    file_count = 1\n",
    "    output_files = []\n",
    "    new_csv_rows = []\n",
    "\n",
    "    current_transcription = {\n",
    "        'eng_reference': [],\n",
    "        'thai_reference': [],\n",
    "        'trimmed_segment_path': None\n",
    "    }\n",
    "\n",
    "    for index, row in csv_df.iterrows():\n",
    "        audio = AudioSegment.from_wav(row['trimmed_segment_path'])\n",
    "\n",
    "        # If adding the next audio exceeds 28 seconds\n",
    "        if len(combined) + len(audio) + padding_duration_ms > max_duration_seconds * 1000:\n",
    "            # Export the current combined audio to a new file\n",
    "            output_filename = f'{output_dir}/combined_output_{file_count}.wav'\n",
    "            combined.export(output_filename, format='wav')\n",
    "            output_files.append(output_filename)\n",
    "            file_count += 1\n",
    "\n",
    "            # Update CSV with the current transcription information\n",
    "            current_transcription['eng_reference'] = ' '.join(current_transcription['eng_reference'])\n",
    "            current_transcription['thai_reference'] = ' '.join(current_transcription['thai_reference'])\n",
    "            current_transcription['trimmed_segment_path'] = output_filename\n",
    "            new_csv_rows.append(current_transcription)\n",
    "\n",
    "            # Start a new combined segment and reset transcription\n",
    "            combined = AudioSegment.silent(duration=0)\n",
    "            current_transcription = {\n",
    "                'eng_reference': [],\n",
    "                'thai_reference': [],\n",
    "                'trimmed_segment_path': None\n",
    "            }\n",
    "\n",
    "        combined += audio + padding\n",
    "        current_transcription['eng_reference'].append(row['eng_reference'])\n",
    "        current_transcription['thai_reference'].append(row['thai_reference'])\n",
    "\n",
    "    # Export the last combined audio segment if it has any content\n",
    "    if len(combined) > 0:\n",
    "        output_filename = f'{output_dir}/combined_output_{file_count}.wav'\n",
    "        combined.export(output_filename, format='wav')\n",
    "        output_files.append(output_filename)\n",
    "\n",
    "        current_transcription['eng_reference'] = ' '.join(current_transcription['eng_reference'])\n",
    "        current_transcription['thai_reference'] = ' '.join(current_transcription['thai_reference'])\n",
    "        current_transcription['trimmed_segment_path'] = output_filename\n",
    "        new_csv_rows.append(current_transcription)\n",
    "\n",
    "    new_csv_df = pd.DataFrame(new_csv_rows)\n",
    "    return output_files, new_csv_df\n",
    "\n",
    "# List of WAV files from the CSV\n",
    "padding_duration_ms = 1000  # 1 second padding\n",
    "max_duration_seconds = 28  # Maximum duration of 28 seconds per file\n",
    "\n",
    "# Combine the WAV files with splitting if necessary\n",
    "output_files, df_train_combined = combine_wav_files_with_split(padding_duration_ms, max_duration_seconds, df_train, \"combined_wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_reference</th>\n",
       "      <th>thai_reference</th>\n",
       "      <th>trimmed_segment_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's been 10 years. Some people can't let go. ...</td>\n",
       "      <td>เหตุการณ์มันผ่านมาแล้ว 10 ปี บางคนปล่อยไม่ได้ ...</td>\n",
       "      <td>combined_wav/combined_output_1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Uh, okay, now. Sometimes. We'll find out. The ...</td>\n",
       "      <td>อะ โอเค ทีนี้ บางครั้ง เราจะเจอแล้วว่า ความคิด...</td>\n",
       "      <td>combined_wav/combined_output_2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We have an N. We have bones. We have muscles. ...</td>\n",
       "      <td>เรามีเอ็น เรามีกระดูก เรามีกล้ามเนื้อ เสียไปทุ...</td>\n",
       "      <td>combined_wav/combined_output_3.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Come into our lives again. Let's just let it g...</td>\n",
       "      <td>เข้ามาในชีวิตของเราอีก ลองปล่อยมันอัตโนมัติไปเ...</td>\n",
       "      <td>combined_wav/combined_output_4.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I know what's out there. I don't know how to s...</td>\n",
       "      <td>รู้แต่เรื่องข้างนอก ไม่รู้จะปิดยังไง เราล่ะ มั...</td>\n",
       "      <td>combined_wav/combined_output_5.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>Leonardo Da Vinci painted it on purpose.  I...</td>\n",
       "      <td>ลีโอนาร์โด ดาร์วินชีเขาตั้งใจวาด เก๋จะขยับกล้อ...</td>\n",
       "      <td>combined_wav/combined_output_680.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>This is the famous painting.    Another highl...</td>\n",
       "      <td>นี่ค่ะรูปที่ดังมากๆ และอีกไฮไลท์นู่น สุดห้องโถ...</td>\n",
       "      <td>combined_wav/combined_output_681.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>Those are the real diamonds.  Don't ask abo...</td>\n",
       "      <td>ที่เห็นนั่นคือเพชรจริงๆ นะคะ อย่าถามราคาว่าเท่...</td>\n",
       "      <td>combined_wav/combined_output_682.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>9,000   It's over 9,000 years. 9,000   This o...</td>\n",
       "      <td>มีอายุกว่า ปี ปีชิ้นนี้ ขอปิดคลิป ด้วยผู้หญิงท...</td>\n",
       "      <td>combined_wav/combined_output_683.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>Chollada Channel   Chollada Channel.  Bye.   ...</td>\n",
       "      <td>กับ ค่ะ บาย อย่าลืมกดไลก์ กดแชร์ หรือว่าซับสไค...</td>\n",
       "      <td>combined_wav/combined_output_684.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>684 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         eng_reference  \\\n",
       "0    It's been 10 years. Some people can't let go. ...   \n",
       "1    Uh, okay, now. Sometimes. We'll find out. The ...   \n",
       "2    We have an N. We have bones. We have muscles. ...   \n",
       "3    Come into our lives again. Let's just let it g...   \n",
       "4    I know what's out there. I don't know how to s...   \n",
       "..                                                 ...   \n",
       "679     Leonardo Da Vinci painted it on purpose.  I...   \n",
       "680   This is the famous painting.    Another highl...   \n",
       "681     Those are the real diamonds.  Don't ask abo...   \n",
       "682   9,000   It's over 9,000 years. 9,000   This o...   \n",
       "683   Chollada Channel   Chollada Channel.  Bye.   ...   \n",
       "\n",
       "                                        thai_reference  \\\n",
       "0    เหตุการณ์มันผ่านมาแล้ว 10 ปี บางคนปล่อยไม่ได้ ...   \n",
       "1    อะ โอเค ทีนี้ บางครั้ง เราจะเจอแล้วว่า ความคิด...   \n",
       "2    เรามีเอ็น เรามีกระดูก เรามีกล้ามเนื้อ เสียไปทุ...   \n",
       "3    เข้ามาในชีวิตของเราอีก ลองปล่อยมันอัตโนมัติไปเ...   \n",
       "4    รู้แต่เรื่องข้างนอก ไม่รู้จะปิดยังไง เราล่ะ มั...   \n",
       "..                                                 ...   \n",
       "679  ลีโอนาร์โด ดาร์วินชีเขาตั้งใจวาด เก๋จะขยับกล้อ...   \n",
       "680  นี่ค่ะรูปที่ดังมากๆ และอีกไฮไลท์นู่น สุดห้องโถ...   \n",
       "681  ที่เห็นนั่นคือเพชรจริงๆ นะคะ อย่าถามราคาว่าเท่...   \n",
       "682  มีอายุกว่า ปี ปีชิ้นนี้ ขอปิดคลิป ด้วยผู้หญิงท...   \n",
       "683  กับ ค่ะ บาย อย่าลืมกดไลก์ กดแชร์ หรือว่าซับสไค...   \n",
       "\n",
       "                     trimmed_segment_path  \n",
       "0      combined_wav/combined_output_1.wav  \n",
       "1      combined_wav/combined_output_2.wav  \n",
       "2      combined_wav/combined_output_3.wav  \n",
       "3      combined_wav/combined_output_4.wav  \n",
       "4      combined_wav/combined_output_5.wav  \n",
       "..                                    ...  \n",
       "679  combined_wav/combined_output_680.wav  \n",
       "680  combined_wav/combined_output_681.wav  \n",
       "681  combined_wav/combined_output_682.wav  \n",
       "682  combined_wav/combined_output_683.wav  \n",
       "683  combined_wav/combined_output_684.wav  \n",
       "\n",
       "[684 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_combined # Trimmed to 684 rows!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting to Huggingface Datasets Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'transcription'],\n",
      "    num_rows: 684\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "audio_dataset_train = Dataset.from_dict({\n",
    "        \"audio\": df_train_combined['trimmed_segment_path'].tolist()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Casting audio column to Audio type\n",
    "audio_dataset_train = audio_dataset_train.cast_column(\"audio\", Audio())\n",
    "\n",
    "# Adding transcriptions column\n",
    "audio_dataset_train = audio_dataset_train.add_column(\"transcription\", np.array(df_train_combined['eng_reference']))\n",
    "\n",
    "print(audio_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'transcription'],\n",
      "    num_rows: 597\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "audio_dataset_test = Dataset.from_dict({\n",
    "        \"audio\": df_test['trimmed_segment_path'].tolist()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Casting audio column to Audio type\n",
    "audio_dataset_test = audio_dataset_test.cast_column(\"audio\", Audio())\n",
    "\n",
    "# Adding transcriptions column\n",
    "audio_dataset_test = audio_dataset_test.add_column(\"transcription\", np.array(df_test['reference']))\n",
    "\n",
    "print(audio_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuser/Desktop/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-medium\", language=\"Thai\", task=\"translate\")\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-medium\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\", language=\"Thai\", task=\"translate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors='pt').input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 684/684 [01:52<00:00,  6.10 examples/s] \n"
     ]
    }
   ],
   "source": [
    "audio_dataset_train = audio_dataset_train.map(\n",
    "    prepare_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 597/597 [00:10<00:00, 57.99 examples/s] \n"
     ]
    }
   ],
   "source": [
    "audio_dataset_test = audio_dataset_test.map(\n",
    "    prepare_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuser/Desktop/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.language = \"thai\"\n",
    "model.generation_config.task = \"translate\"\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        # return in pytorch tensors\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length and return in pytorch tensors\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-medium-thai\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=1000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=150,\n",
    "    save_steps=200,\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=audio_dataset_train,\n",
    "    eval_dataset=audio_dataset_test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "  0%|          | 3/1000 [00:14<1:19:13,  4.77s/it]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 15.70 GiB total capacity; 15.06 GiB already allocated; 8.00 MiB free; 15.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/.venv/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/.venv/lib/python3.10/site-packages/transformers/trainer.py:2266\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2263\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[0;32m-> 2266\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2267\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n\u001b[1;32m   2268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run:\n\u001b[1;32m   2269\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/.venv/lib/python3.10/site-packages/accelerate/optimizer.py:157\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_patched_step_method\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called:\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;66;03m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/.venv/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:374\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 374\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/Desktop/.venv/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:290\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 290\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/Desktop/.venv/lib/python3.10/site-packages/accelerate/optimizer.py:212\u001b[0m, in \u001b[0;36mpatch_optimizer_step.<locals>.patched_step\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpatched_step\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    211\u001b[0m     accelerated_optimizer\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/Desktop/.venv/lib/python3.10/site-packages/torch/optim/adamw.py:171\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    161\u001b[0m         group,\n\u001b[1;32m    162\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m         state_steps,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Desktop/.venv/lib/python3.10/site-packages/torch/optim/adamw.py:321\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 321\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/.venv/lib/python3.10/site-packages/torch/optim/adamw.py:566\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    564\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_exp_avg_sqs)\n\u001b[1;32m    565\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m--> 566\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg_sq_sqrt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(device_params, device_exp_avgs, denom, step_size)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 15.70 GiB total capacity; 15.06 GiB already allocated; 8.00 MiB free; 15.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [02:09<00:00,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4484760761260986, 'eval_wer': 88.11989100817439, 'eval_runtime': 131.0741, 'eval_samples_per_second': 4.555, 'eval_steps_per_second': 0.572, 'epoch': 33.11258278145695}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_logs = trainer.evaluate()\n",
    "print(eval_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"whisper-small-thai/checkpoint-1000\").to(\"cuda\")\n",
    "#processor = WhisperProcessor.from_pretrained(\"whisper-medium-thai/checkpoint-5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'transcription', 'input_features', 'labels'],\n",
       "    num_rows: 597\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_pred(batch):\n",
    "\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "\n",
    "    batch[\"reference\"] = processor.tokenizer._normalize(batch['transcription'])\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n",
    "\n",
    "    transcription = processor.decode(predicted_ids)\n",
    "\n",
    "    batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuser/Desktop/.venv/lib/python3.10/site-packages/transformers/models/whisper/tokenization_whisper.py:500: UserWarning: The private method `_normalize` is deprecated and will be removed in v5 of Transformers.You can normalize an input string using the Whisper English normalizer using the `normalize` method.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 597/597 [01:44<00:00,  5.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "result = audio_dataset_test.map(map_to_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113.06244886828469\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "wer = load(\"wer\")\n",
    "\n",
    "print(100 * wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>it is now 4 pm</th>\n",
       "      <td>right now i m in singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>singaporean love</th>\n",
       "      <td>most people when they come to singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>let us have a 3 on 3</th>\n",
       "      <td>they come to eat dim sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>let us see what is on the menu</th>\n",
       "      <td>visit merlion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i am in verso</th>\n",
       "      <td>visit universal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>please like share and subscribe for me</th>\n",
       "      <td>please press like share and subscribe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>who wants me to go on a trip</th>\n",
       "      <td>if you want me to take you somewhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how do you play next</th>\n",
       "      <td>to eat or do any activities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you can comment</th>\n",
       "      <td>you can leave me a comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>please leave a comment</th>\n",
       "      <td>until next time bye bye</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>597 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              0\n",
       "it is now 4 pm                                       right now i m in singapore\n",
       "singaporean love                        most people when they come to singapore\n",
       "let us have a 3 on 3                                   they come to eat dim sum\n",
       "let us see what is on the menu                                    visit merlion\n",
       "i am in verso                                                   visit universal\n",
       "...                                                                         ...\n",
       "please like share and subscribe for me    please press like share and subscribe\n",
       "who wants me to go on a trip               if you want me to take you somewhere\n",
       "how do you play next                                to eat or do any activities\n",
       "you can comment                                      you can leave me a comment\n",
       "please leave a comment                                  until next time bye bye\n",
       "\n",
       "[597 rows x 1 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result['reference'], result['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
