This project aims to address the challenges of transcribing and translating audio from low-resource languages by leveraging the capabilities of large language models (LLMs) in a multimodal setting. By integrating the ASR and LLM with an projection adaptor, we sought to develop a novel solution for translating conversational low-resource languages. Our experiments have demonstrated the viability of using LLMs for direct audio-to-text translation in low-resource languages settings. We hope our work serves as a starting point for further exploration and improvement in this field, ultimately contributing to more accessible and accurate translation technologies for low-resource languages.
