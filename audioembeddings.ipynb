{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuser/Desktop/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-20 10:05:56.658096: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-20 10:05:56.691374: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-20 10:05:57.233026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, WhisperModel\n",
    "import torch\n",
    "import wave\n",
    "\n",
    "# Step 1: Load the Audio Data\n",
    "path_to_audio = \"data/sub/De95Osq7p1c_trimmed_segment_1.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "\n",
    "def read_wav_file(file_path):\n",
    "    # Open the WAV file\n",
    "    with wave.open(file_path, 'rb') as wav_file:\n",
    "        # Get the number of frames in the file\n",
    "        n_frames = wav_file.getnframes()\n",
    "        \n",
    "        # Read the frame data\n",
    "        frame_data = wav_file.readframes(n_frames)\n",
    "        \n",
    "        # Get the sample width (in bytes)\n",
    "        sample_width = wav_file.getsampwidth()\n",
    "        \n",
    "        # Get the number of channels\n",
    "        n_channels = wav_file.getnchannels()\n",
    "        \n",
    "        # Get the frame rate (samples per second)\n",
    "        frame_rate = wav_file.getframerate()\n",
    "        \n",
    "        # Convert the byte data to a numpy array\n",
    "        if sample_width == 1:\n",
    "            # 8-bit audio\n",
    "            audio_data = np.frombuffer(frame_data, dtype=np.uint8)\n",
    "        elif sample_width == 2:\n",
    "            # 16-bit audio\n",
    "            audio_data = np.frombuffer(frame_data, dtype=np.int16)\n",
    "        elif sample_width == 4:\n",
    "            # 32-bit audio\n",
    "            audio_data = np.frombuffer(frame_data, dtype=np.int32)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported sample width: {}\".format(sample_width))\n",
    "        \n",
    "        # Reshape the array based on the number of channels\n",
    "        if n_channels > 1:\n",
    "            audio_data = audio_data.reshape(-1, n_channels)\n",
    "        \n",
    "        return audio_data, frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Data Shape: (54864,)\n",
      "Frame Rate: 16000\n"
     ]
    }
   ],
   "source": [
    "audio_data, frame_rate = read_wav_file(path_to_audio)\n",
    "\n",
    "print(\"Audio Data Shape:\", audio_data.shape)\n",
    "print(\"Frame Rate:\", frame_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuser/Desktop/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    audio_data,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    "    sampling_rate=16000     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': tensor([[[1.4301, 1.4301, 1.4301,  ..., 1.4301, 1.4301, 1.4301],\n",
       "         [1.4301, 1.4301, 1.4301,  ..., 1.4301, 1.4301, 1.4301],\n",
       "         [1.4301, 1.4301, 1.4301,  ..., 1.4301, 1.4301, 1.4301],\n",
       "         ...,\n",
       "         [1.4301, 1.4301, 1.4301,  ..., 1.4301, 1.4301, 1.4301],\n",
       "         [1.4301, 1.4301, 1.4301,  ..., 1.4301, 1.4301, 1.4301],\n",
       "         [1.4301, 1.4301, 1.4301,  ..., 1.4301, 1.4301, 1.4301]]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Audio with Whisper Encoder\n",
    "model = WhisperModel.from_pretrained(\"openai/whisper-medium\")\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Ensuring No Gradient Updates\n",
    "with torch.no_grad():\n",
    "    encoder_outputs = model.encoder(**inputs, output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of audio embeddings: torch.Size([1, 1500, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Retrieve the Hidden States\n",
    "hidden_states = encoder_outputs.hidden_states\n",
    "audio_embeddings = encoder_outputs.last_hidden_state\n",
    "\n",
    "print(\"Shape of audio embeddings:\", audio_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuser/Desktop/.venv/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:473: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"<|startoftranscript|><|en|><|transcribe|><|notimestamps|> I'm gonna make a new friend!<|endoftext|>\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model_generate = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")\n",
    "model_generate.to(device)\n",
    "\n",
    "generated = model_generate.generate(inputs = inputs.input_features)\n",
    "processor.batch_decode(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting Audio Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptor(nn.Module):\n",
    "    def __init__(self, output_embedding_size = 4096):\n",
    "        super().__init__()\n",
    "        self.pool1 = torch.nn.AdaptiveAvgPool1d(256)\n",
    "        self.linear = torch.nn.Linear(1280, output_embedding_size, bias=False)\n",
    "        self.ln1 = torch.nn.LayerNorm(1280)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
