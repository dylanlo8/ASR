{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, WhisperModel, AutoTokenizer\n",
    "import torch\n",
    "import wave\n",
    "\n",
    "# Load the Audio Data\n",
    "path_to_audio = \"data/sub/De95Osq7p1c_trimmed_segment_1.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "\n",
    "def read_wav_file(file_path):\n",
    "    # Open the WAV file\n",
    "    with wave.open(file_path, 'rb') as wav_file:\n",
    "        # Get the number of frames in the file\n",
    "        n_frames = wav_file.getnframes()\n",
    "        \n",
    "        # Read the frame data\n",
    "        frame_data = wav_file.readframes(n_frames)\n",
    "        \n",
    "        # Get the sample width (in bytes)\n",
    "        sample_width = wav_file.getsampwidth()\n",
    "        \n",
    "        # Get the number of channels\n",
    "        n_channels = wav_file.getnchannels()\n",
    "        \n",
    "        # Get the frame rate (samples per second)\n",
    "        frame_rate = wav_file.getframerate()\n",
    "        \n",
    "        # Convert the byte data to a numpy array\n",
    "        if sample_width == 1:\n",
    "            # 8-bit audio\n",
    "            audio_data = np.frombuffer(frame_data, dtype=np.uint8)\n",
    "        elif sample_width == 2:\n",
    "            # 16-bit audio\n",
    "            audio_data = np.frombuffer(frame_data, dtype=np.int16)\n",
    "        elif sample_width == 4:\n",
    "            # 32-bit audio\n",
    "            audio_data = np.frombuffer(frame_data, dtype=np.int32)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported sample width: {}\".format(sample_width))\n",
    "        \n",
    "        # Reshape the array based on the number of channels\n",
    "        if n_channels > 1:\n",
    "            audio_data = audio_data.reshape(-1, n_channels)\n",
    "        \n",
    "        return audio_data, frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Data Shape: (54864,)\n",
      "Frame Rate: 16000\n"
     ]
    }
   ],
   "source": [
    "audio_data, frame_rate = read_wav_file(path_to_audio)\n",
    "\n",
    "print(\"Audio Data Shape:\", audio_data.shape)\n",
    "print(\"Frame Rate:\", frame_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuser/Desktop/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    audio_data,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    "    sampling_rate=16000     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': tensor([[[1.4301, 1.4301, 1.4301,  ..., 1.4301, 1.4301, 1.4301],\n",
       "         [1.4301, 1.4301, 1.4301,  ..., 1.4301, 1.4301, 1.4301],\n",
       "         [1.4301, 1.4301, 1.4301,  ..., 1.4301, 1.4301, 1.4301],\n",
       "         ...,\n",
       "         [1.4301, 1.4301, 1.4301,  ..., 1.4301, 1.4301, 1.4301],\n",
       "         [1.4301, 1.4301, 1.4301,  ..., 1.4301, 1.4301, 1.4301],\n",
       "         [1.4301, 1.4301, 1.4301,  ..., 1.4301, 1.4301, 1.4301]]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Audio with Whisper Encoder\n",
    "model = WhisperModel.from_pretrained(\"openai/whisper-medium\")\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Ensuring No Gradient Updates\n",
    "with torch.no_grad():\n",
    "    encoder_outputs = model.encoder(**inputs, output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of audio embeddings: torch.Size([1, 1500, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Retrieve the Hidden States\n",
    "hidden_states = encoder_outputs.hidden_states\n",
    "audio_embeddings = encoder_outputs.last_hidden_state\n",
    "\n",
    "print(\"Shape of audio embeddings:\", audio_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8103,  0.5628,  0.3018,  ...,  1.4972, -1.0344,  0.3765],\n",
       "         [ 0.0931,  0.2974, -0.1004,  ...,  0.2545, -0.1267, -0.6107],\n",
       "         [ 0.7071, -0.0789,  0.9369,  ..., -0.7768,  0.7219, -0.7023],\n",
       "         ...,\n",
       "         [ 0.1007,  1.0809,  1.3225,  ...,  1.2200, -0.2207,  0.1168],\n",
       "         [-0.4745,  0.9073,  1.8731,  ...,  1.6187, -0.3987,  0.0856],\n",
       "         [-0.4203,  1.0367,  2.1674,  ...,  1.8687, -0.3581,  0.1691]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuser/Desktop/.venv/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:473: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"<|startoftranscript|><|en|><|transcribe|><|notimestamps|> I'm gonna make a new friend!<|endoftext|>\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "model_generate = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")\n",
    "model_generate.to(device)\n",
    "\n",
    "generated = model_generate.generate(inputs = inputs.input_features)\n",
    "processor.batch_decode(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting Adaptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptor(torch.nn.Module):\n",
    "    # A simple learnable adaptor\n",
    "    def __init__(self, output_embedding_size = 1024): # Bloom's Embedding size for each token\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(1024, output_embedding_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioToTextPipeline(nn.Module):\n",
    "    def __init__(self, whisper_model_name=\"openai/whisper-medium\", bloom_model_name=\"bigscience/bloom-560m\"):\n",
    "        super(AudioToTextPipeline, self).__init__()\n",
    "        \n",
    "        # Load the Whisper model and processor\n",
    "        self.processor = AutoProcessor.from_pretrained(whisper_model_name)\n",
    "        self.whisper_model = WhisperModel.from_pretrained(whisper_model_name)\n",
    "        \n",
    "        # Freeze Whisper weights\n",
    "        for param in self.whisper_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Load the Bloom model and tokenizer\n",
    "        self.bloom_model = BloomForCausalLM.from_pretrained(bloom_model_name)\n",
    "        self.tokenizer = BloomTokenizerFast.from_pretrained(bloom_model_name)\n",
    "        \n",
    "        # Freeze Bloom weights\n",
    "        for param in self.bloom_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Define the Adaptor\n",
    "        self.adaptor = torch.nn.Linear(1024, 1024, bias=False)\n",
    "\n",
    "    def forward(self, audio_file_path):\n",
    "        # Step 1: Generate audio embeddings\n",
    "        inputs = self.processor(audio_file_path, return_tensors=\"pt\", sampling_rate=16000)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.whisper_model(**inputs)\n",
    "        audio_embeddings = outputs.last_hidden_state.squeeze(0)  # Assuming batch size of 1\n",
    "\n",
    "        # Step 2: Transform embeddings using the adaptor\n",
    "        transformed_embeddings = self.adaptor(audio_embeddings)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bloom_model(inputs_embeds=transformed_embeddings.unsqueeze(0))\n",
    "        \n",
    "        # generated_tokens = outputs.logits.argmax(dim=-1)\n",
    "        generated_text = self.tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        \n",
    "        return generated_text\n",
    "\n",
    "    def training_step(self, audio_file_path, target_text):\n",
    "        # Forward pass\n",
    "        inputs = self.processor(audio_file_path, return_tensors=\"pt\", sampling_rate=16000)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.whisper_model(**inputs)\n",
    "        audio_embeddings = outputs.last_hidden_state.squeeze(0)  # Assuming batch size of 1\n",
    "        \n",
    "        # Transform embeddings using the adaptor\n",
    "        transformed_embeddings = self.adaptor(audio_embeddings)\n",
    "        \n",
    "        # Prepare input for the Bloom model\n",
    "        input_ids = torch.zeros((1, transformed_embeddings.size(0)), dtype=torch.long)  # Dummy input IDs\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Generate predictions\n",
    "        outputs = self.bloom_model(input_ids=input_ids, attention_mask=attention_mask, inputs_embeds=transformed_embeddings.unsqueeze(0))\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Tokenize target text\n",
    "        target_ids = self.tokenizer(target_text, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "\n",
    "        # Calculate loss\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = target_ids[..., 1:].contiguous()\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train_model(self, train_data, epochs=1, learning_rate=1e-3):\n",
    "        # Only optimize the adaptor's parameters\n",
    "        optimizer = optim.Adam(self.adaptor.parameters(), lr=learning_rate)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for audio_file_path, target_text in train_data:\n",
    "                optimizer.zero_grad()\n",
    "                loss = self.training_step(audio_file_path, target_text)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_data)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
